<html><head>
	<title>Better Food Recommendations - How It Works</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="/style.css" rel="stylesheet" type="text/css">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
	<link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;600&amp;display=swap" rel="stylesheet">
</head>

<body>
	<div class="header header-short">
		<div class="header-bar">
            <div class="nav-item"><a href="/" style="text-decoration: none;"><div class="site-title">BetterFoodRecs</div></a>
        </div><div class="nav-item nav-menu"><a href="/about" class="nav-menu-item">About</a><a href="/workings" class="nav-menu-item">How it Works</a></div></div>
		<div class="header-input-section">
			<div class="header-title">
				<h1 id="query-text">How It Works</h1>
			</div>
		</div>
	</div>
  
<div class="container">
    <div class="body-text">
      
      <p class="body-p">Here's the fun part.</p>
      
    <h1 class="body-h">Outlining</h1><p class="body-p">There are four components to pipeline necessary to make this work:</p><ul class="body-p"><li>Locating and Parsing Reddit Data</li>
<li>Extracting Restaurant Names and Sentiment</li>
<li>Computing a Popularity Score</li>
<li>Clustering Restaurant Results</li></ul><img src="/workflow.png" class="body-img"><h1 class="body-h">Locating &amp; Parsing Data</h1><p class="body-p">The first order to business is locating relevant threads to extract comments from. For this, we can pull results from Google, domain limited to reddit.com. While Reddit has a native search functionality, it's not nearly as advanced when it comes to turning up relevant threads. For example: <a href="https://www.reddit.com/search/?q=best%20los%20angeles%20food">Reddit</a> vs <a href="https://www.google.com/search?q=best+los+angeles+food+reddi">Google</a></p><p class="body-p">We'll take the top 6 links, which typically provides sufficient data.</p><p class="code-p">request <span class="blue">=</span> requests.<span class="purple">get</span>(<span class="dblue">f"https://www.googleapis.com/customsearch/v1?key={API_KEY}&amp;cx={SEARCH_ENGINE_ID}&amp;q={query}"</span>).<span class="purple">json</span>()<br><span class="orange">for</span> i, search_item <span class="blue">in</span> <span class="purple">enumerate</span>(request['items'][:<span class="blue">6</span>], start=1):
<br>     # extract comments<br><small>(Pseudo-code)</small></p><p class="body-p">For here, we'll want to get a list of comments for each thread. Reddit previder their data as JSON via API, so this is simple to do. We'll also want to run some pre-processing to ensure any unwanted text is stripped out, and newlines are replaced with periods to help with latter processing.</p><p class="code-p"> <span class="red">class</span> <span class="orange">RedditParser</span>():
<br><br>
<span class="red">&nbsp;&nbsp;&nbsp;&nbsp;
def</span> <span class="purple">__init__</span>(self, url):<br>&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;

    self.url = url + ( "reddit.json" if url[-1] == "/" else "/reddit.json" )<br>

    <br><span class="red">&nbsp;&nbsp;&nbsp;&nbsp;
def</span> <span class="purple">__parse_c</span>(self, string):<br>
    <span class="green">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Strip out newlines, replace them with a period, and strip out and repeating spaces.<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Remove markdown links, and run preprocessing \<br>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""
    <br></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;string = re.<span class="purple">sub</span>(<span class="darkblue">(?:\n|\r|\\n|\\r)+"</span>, ". ", re.sub(<span class="darkblue">r"(\.|\?|!|:) *(?:\n|\r|\\n|\\r)+"</span>, r"\1 ", string))<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;string = re.<span class="purple">sub</span>(<span class="darkblue">r'\[[^\]]+\]\([^)]+\)'</span>, "", string)<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="red">return</span> string.<span class="purple">replace</span>('&amp;', '&amp;').<span class="purple">replace</span>('\\', "").<span class="purple">replace</span>("\t", "").<span class="purple">replace</span>('’', "'")<br><br>

    <span class="red">&nbsp;&nbsp;&nbsp;&nbsp;def</span> <span class="purple">__rec_pull_replies</span>(self, comment, comments):<br>
    <span class="red">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for</span> i <span class="blue">in</span> comment['replies']['data']['children']:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;comments.<span class="purple">append</span>((self.__parse_c(i['data']['body']))<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.<span class="purple">__rec_pull_replies</span>(i['data'], comments)<br></p><p class="body-p">Once this is all done, we'll have a list of comments that looks like this:</p><p class="code-p">[<br>
    &nbsp;&nbsp;&nbsp;&nbsp;{<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"body": "X sucks, I went there last week",<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"upvotes": 4,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"date": "1557724800",<br>
    &nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;...<br>
    ]</p><h1 class="body-h">Restaurant Names And Sentiment</h1><p class="body-p">For each restauraunt, it's important to determine if a comment is recommending it, or recommending <i>against</i> it.</p><p class="body-p">To do so, we'll need to find the sentiment of the comment <i>towards</i> the entity. Both named-entity-recognition and sentiment analysis are very popular tasks - however, entity-sentiment analysis is significantly less popular and common libraries don't provide such functionality. (Google's NLP API provides entity sentiment detection, but is cost-prohibitive).</p>
<p class="body-p">Past studies have used models trained against certain entites to detect sentiment (<a href="https://dl.acm.org/doi/abs/10.1145/2339530.2339592">Meng et al</a>). Other approaches combining entity detection using a variety of approaches have also been proposed before, acheiving ~60-80% (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0306457315000242">Saif et al</a>, <a href="https://dl.acm.org/doi/abs/10.1145/1988688.1988703">Engonopoulos et al</a>). Such approaches however, are not easily accessible as they require either a pre-targeted list of entities or significant training / corpus data.</p><p class="body-p">More primitive methods involved breaking up NER (entity recognition) and sentiment classification into two separate tasks (<a href="https://nlp.stanford.edu/courses/cs224n/2010/reports/drao-sidbatra.pdf">Batra et al</a>). For our purposes, this approach will suffice. </p><p class="body-p">To do so, we can extract all the entities from a comment, filter out irrelevant ones (ie, locations), and tag them with the sentiment of the comment. However, since comments can contain both a pro-recommendation and an against recommendation, we'll take the sentence-level sentiment when tagging an enttiy. This reduces scenerios where entties are improperly tagged. Fortunately for us, restaurant recommendations typically follow similiar formats, making this approach relatively robust, though the trade-off is that we potentially miss against-recommendations that elaborate in further sentences.</p><p class="body-p">Because people ask for recommendations (most people don't post asking about restaurants to avoid), we'll assume any "neutral" mention is a positive vote too.</p><p class="code-p">&gt; Don't go to Mulberry. The food sucks. Pizzana much is better.<br><b>Mulberry:</b> Sentence Level - Negative, Comment Level - Negative<br><b>Pizzana:</b> Sentence Level - Positive, Comment Level - Negative<br><br>&gt; I love you for this - thank you! My friend said that the lobster mash at Mastro’s is a bad lol.<br><b>Mastro’s:</b> Sentence Level - Positive, Comment Level - Negative<br><br>&gt; Well, remember it's also about what YOU like. I think Ruth Chris' steaks SUCK. If I wanted to eat charcoal, I'd eat charcoal. Me? I like Taylor's Steak House. Nice place and great steaks.<br><b>Ruth Chris:</b> Sentence Level - Negative, Comment Level - Negative<br><b>Taylor's Steak House:</b> Sentence Level - Positive, Comment Level - Negative<br><br>&gt; I was at Mastro's last night actually. There's no way that's the best in the city. I mean, it's far from terrible but also far from amazing. See if you can get someone to get you in at magic castle.<br><b>(Incorret) Mastro’s:</b> Sentence Level - Positive, Comment Level - Negative</p><p class="body-p">For sentiment recognition, a relatively simple task, we'll use a RoBERTa based model trained on Twitter data, which provides ~95% accuracy.</p><p class="body-p">Named Entity Recognition in itself is typically a simple task. However, we want to ensure any entities we extract are restaurants, not people, locations, or other entities.</p><p class="body-p">One approach to this is to check against a comprehensive database of restaurants, such as Google Map's API. However, doing so is cost-prohibitive, and also extremely slow (it's normal for 500 comments to have 750+ entities to check). Instead, with some model ensembling and filtering, we can devise a system to accurately extract restaurants, and restaurants only. </p><p class="body-p">In our case, we used Spacy's RoBERTA Transformer NLP model paired with Flair's Large NER model to cross-check any extracted entities. The entities we are looking for are: 'PERSON', 'FAC', 'ORG', 'WORK_OF_ART', 'PRODUCT.' To solve any disagreements between the two models, we can use Spacy's dependency parser look at syntax to pull certain subjects (ie, nsubj, dobj) in a sentence and cross-reference those with any entities picked up by either model. If 2/3 methods agree, an entity is added to the final list.</p><p class="body-p">Naturally, this method does flag create false positives and negatives. Therefore, we made the assumption that if a restaurant is truly relevant, it'll appear enough times for the model to pick it up, and if something that isn't a restaurant is picked up, it won't appear enough times to throw off the results greatly. Because of this margin of error, restaurants below a certain score threshold may not actually be restaurants, or might not have an accurate representation of popularity.</p><p class="code-p"><span class="red">for</span> z <span class="blue">in</span> <span class="purple">range</span>(<span class="purple">len</span>(spacy_sents)):<br>
    &nbsp;&nbsp;&nbsp;&nbsp;locations <span class="blue">=</span> self.<span class="purple">__verify_entity_position</span>(self.<span class="purple">__extract_nsubj_dobj_spacy</span>(spacy_sents[z]), self.<span class="purple">__extract_notable_entities_spacy</span>(spacy_sents[z].ents), self.<span class="purple">__extract_notable_entities_flair</span>(flair_sents[i].<span class="purple">get_spans</span>('ner')))<br>
    &nbsp;&nbsp;&nbsp;&nbsp;additonal_locations <span class="blue">=</span> self.<span class="purple">__verify_entity_position</span>(self.<span class="purple">__extract_nsubj_dobj_spacy</span>(spacy_sents[z]), self.<span class="purple">__extract_notable_entities_flair</span>(flair_sents[i].<span class="purple">get_spans</span>('ner')), [])<br>
    &nbsp;&nbsp;&nbsp;&nbsp;locations <span class="blue">+=</span> additonal_locations</p><p class="body-p">We end up with a list like this (after we cluster names. More on that later):</p><p class="code-p">[<br>
    &nbsp;&nbsp;&nbsp;&nbsp;{<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Mastro": -1,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Pizza": 1,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;{<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Mozzo": 1,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;},<br>&nbsp;&nbsp;&nbsp;&nbsp;...<br>
    ]</p><h1 class="body-h">Computing a Popularity Score</h1><p class="body-p">Now that we have the essential data needed, we can proceed with computing a popularity score for each restaurant based on the # of upvotes, the age of the comment, and sentiment towards a restaurant.</p><p class="body-p">We'll want to add a time decay to the final score. A 10-year old comment won't be as relevant as a 5 month old comment, as food changes. However, this decay should not be linear. Instead, we'll use a exponential decay curve.</p><p class="code-p"><span class="red">min</span>(1, 1 / (0.005*(years_ago-66)) + 4.08) </p><p class="body-p">We'll also want to flatten the upvote score, similiar to Reddit's karma system, to prevent one single, highly upvoted comment from masking everything else. Doing so also reduces the risk that a miscategorized restaurant does not have as big of an impact as the rest of the listings. Here, we use an logarithmic function.</p><p class="code-p">3 * math.<span class="purple">log</span>(<span class="purple">abs</span>(raw) + 1, 10) * (1 <span class="red">if</span> raw &gt; 0 <span class="red">else</span> -1)</p><p class="body-p">Since we're taking the absolute value when applying the log, we'll restore the original sign when returning the function. Finally, we just multiply everything together.</p><p class="code-p">time_decay * upvote_score * restaurant_sentiment</p><h1 class="body-h">Clustering Restaurant Results</h1><p class="body-p">Possible one of the most complex challenges is ensuring that restaurants with similiar names aren't counted twice, or separately. "Subway," "Sub way," and "Subway Sandwich" refer to the same restaurant.</p><p class="body-p">There are several approaches to this. For one, we can apply several text-similarity algorithms (Hamming, Jaro, Levenshtein distance) or even cosine similiarity with GloVE vectors. One issue with Hamming/Jaro/Levenshtein distance (which looks at the number of operations needed to make string_a into string_b) is that the phrase "Alexander's Steakhouse" and "Alexander's" would have a high distance (11), meaning the threshold would need to be set abnormally high to catch all forms of restaurant names.</p><p class="body-p">If we want to get even more complex, K-Means Clustering, or better, Affinity Propagation, could be suitable for this task if pair with some  distance algorithm. </p><p class="body-p">I found that Fuzzywuzzy's partial_ratio() function (which implements Ratcliff-Obershelp via SequenceMatcher) with minor modifications is generally sufficient. We'll look at the longest result in each "cluster" of restaurants to decide if a name should be added to a cluster, and applying some minor transformations - lower-casing and inversion.
</p><p class="code-p">        fuzz.<span class="purple">partial_ratio</span>(g_longest.<span class="purple">lower</span>(), k.<span class="purple">lower</span>()) &gt; 80 or fuzz.<span class="purple">partial_ratio</span>(g_longest.<span class="purple">lower</span>(), ' '.join(k_rev).<span class="purple">lower</span>()) &gt; 90


</p><p class="body-p">This clustering needs to be applied in two places - once, when parsing a comment, so ensure a restaurant is not counted twice in a comment. Another time, when aggregating the scores across multiple comments. In the former case, we simply drop the extra names, while in the latter case, we'll add up the scores.
</p><p class="body-p">We end with something like this:
</p><p class="code-p">{<br>
    &nbsp;&nbsp;&nbsp;&nbsp;"Mastro": ["Mastro","Mastro's","Mastros"],<br>
    &nbsp;&nbsp;&nbsp;&nbsp;"Subway": ["Subway sandwich","subway","the subway"],<br>
    &nbsp;&nbsp;&nbsp;&nbsp;"Bread &amp; Butter": ["Bread and butter","bread butter","Bread an Butter"],<br>&nbsp;&nbsp;&nbsp;&nbsp;...<br>}</p><h1 class="body-h">Presenting Data</h1><p class="body-p">Finally, it's just a matter of pairing the aggregated restaurant names with their respective scores, a trivial task. We've now tied together a complete pipeline that processes each post and returns a list of recommendations, which we can serve via an API.</p><p class="body-p">On the client side, it's a simple call to an external API to retrieve restaurant names from an actual database and populate the listing with business status, and address.</p><p class="code-p"><span class="purple">fetch</span>(<span class="blue">`https://api.betterfoodrecs.com/api/v1/invoke/?name=</span>${item[0]}<span class="blue">&amp;location=</span>${location_c}<span class="blue">&amp;food=</span>${food_c}<span class="blue">`</span>).<span class="purple">then</span>( <br>
        &nbsp;&nbsp;&nbsp;&nbsp;(response) =&gt; response.json() <br>
        ).<span class="purple">then</span>(<br>
        &nbsp;&nbsp;&nbsp;&nbsp;(data) =&gt; {<br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="red">if</span> (data['results'][0]['name'].length &gt; 1){<br>
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="purple">populate_data</span>(data);<br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
                &nbsp;&nbsp;&nbsp;&nbsp;}<br>
        )<br></p><p class="body-p">Built by Welton Wang © 2021</p></div>
	</div>

</body></html>
